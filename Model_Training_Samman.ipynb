{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HLNoZfkBUGc"
      },
      "source": [
        "# **1-Import library**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UkuJaCW9bT6u"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "import json\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
        "from tensorflow.keras.optimizers import SGD,Adam\n",
        "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
        "import random\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TbN0QbsScujH",
        "outputId": "9011ddf6-5c1b-4674-cf17-747317203e84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "slixlMlwdJji",
        "outputId": "ad7764be-f28b-428d-a28b-185d66611a67"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.15.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "tf.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpC21eawvwSY"
      },
      "source": [
        "#**2-Dataset Loading**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R60NmwbGDNp2",
        "outputId": "3cd92783-cda1-4126-e3b7-8f91c099ae0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CfFYn2f_b5u9",
        "outputId": "5a52e3ae-26e8-4079-8dc5-13d0c44ecb07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing the intents..........\n"
          ]
        }
      ],
      "source": [
        "words=[]\n",
        "classes = []\n",
        "documents = []\n",
        "ignore_words = ['?', '!',\"'\",]\n",
        "print(\"Processing the intents..........\")\n",
        "path_to_save_model =\"/content/gdrive/MyDrive/TrainedModel/\"\n",
        "with open('/content/gdrive/MyDrive/Dataset/intentsnew (6).json') as json_data:\n",
        "  intents = json.load(json_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Yl5GZnxZnI3"
      },
      "source": [
        "# **3-Data Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ICA7KJTRD768",
        "outputId": "1722c459-02b2-41ea-9f98-3da8a03718f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looping through intents to convert them to words, classes, documents and ignore_words.........\n"
          ]
        }
      ],
      "source": [
        "print(\"Looping through intents to convert them to words, classes, documents and ignore_words.........\")\n",
        "for intent in intents['intents']:\n",
        "    for pattern in intent['patterns']:\n",
        "        #tokenize each word in the sentence\n",
        "        w = nltk.word_tokenize(pattern)\n",
        "        #add to our words list\n",
        "        words.extend(w)\n",
        "        # add to documents in the corpus\n",
        "        documents.append((w, intent['tag']))\n",
        "        # add to our classes list\n",
        "        if intent['tag'] not in classes:\n",
        "            classes.append(intent['tag'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-azsX0L8c7hX",
        "outputId": "9ff7e22f-b748-4416-b4fb-323564b5277f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "305 documents\n",
            "113 classes ['BCPLstand', 'C++', 'Campus changed', 'CandC++', 'Credit', 'Dev C++', 'IDE', 'Introduction C Programming', 'Introduction to  C/C+ Programming', 'LecCS201CS201P', 'OOP', 'Practical', 'Result', 'VSIDE', 'about', 'academic_support', 'addordropcourses', 'admission', 'admission_criteria', 'admission_process', 'admission_types', 'appearforexam', 'askquestionfromteacher', 'assignments', 'avoid_plagiarism', 'benefits', 'bestprogramlanguage', 'campuschange', 'career_services', 'cgpa', 'comparablecourses', 'complaint', 'concerncertificate', 'contact_professor', 'contactadmissiondepartment', 'continueeducationaftercompletingdefree', 'course selection', 'coursesyllabus', 'create account', 'datasavedisk', 'defragmentation', 'downloaddev', 'duplicateid', 'exam_system', 'exam_type', 'expensiveeducation', 'faculty_contact', 'fee_deadline', 'fee_installments', 'fee_structure', 'finalterm', 'financial_aid', 'fragmentation', 'freezeaccount', 'functions', 'functions type', 'functon declaration', 'gdIDE', 'gdb', 'goodbye', 'gpa', 'grading_scheme', 'greetings', 'help', 'historycinfluencelanguage', 'historyprogramlanguage', 'how_to_study', 'howtostudy', 'importanceprogram', 'improvecod', 'institution', 'language', 'lecturehandoutsanddvd', 'lecwat', 'library', 'logicop', 'midterm', 'migrationcertificate', 'mistakeexpression', 'name', 'online_learning', 'operatecomputer', 'passing_criteria', 'payment_options', 'physical session', 'physical_campus', 'plagiarism', 'plagiarism_consequences', 'professor_feedback', 'professors', 'programmingtough', 'quizzes', 'recognizeddegree', 'social', 'standardsofeducation', 'student status Overseas to Pakistan and Vice Versa', 'student_support', 'studentstatusupdation', 'study_tips', 'support_services', 'support_system', 'technical_support', 'term', 'thanks', 'time_management', 'twaindrivers', 'uniformeducation', 'virtual_campus', 'vu_campuses', 'vu_cost', 'vu_professors', 'wheretostudy', 'writeprogram']\n",
            "391 unique lemmatized words ['&', \"''\", \"'m\", \"'s\", '(', ')', ',', '10', '``', 'a', 'able', 'about', 'academic', 'account', 'activity', 'add', 'admission', 'advantage', 'after', 'aid', 'algorithm', 'all', 'am', 'an', 'and', 'any', 'appear', 'apply', 'appreciate', 'are', 'ask', 'assignment', 'assistance', 'at', 'attend', 'attending', 'available', 'avoid', 'avoiding', 'basic', 'bcpl', 'be', 'benefit', 'best', 'better', 'between', 'both', 'but', 'by', 'bye', 'c', 'c++', 'calculated', 'call', 'campus', 'can', 'card', 'career', 'caught', 'certificate', 'certificate/no', 'cgpa', 'change', 'changed', 'choose', 'class', 'code', 'coding', 'common', 'comparable', 'complaint', 'completing', 'computer', 'concern', 'conducted', 'consequence', 'contact', 'content', 'contents/syllabus', 'continue', 'cost', 'could', 'country', 'course', 'cpp', 'create', 'credit', 'criterion', 'cs201', 'cs201p', 'daily', 'data', 'deadline', 'deal', 'declaration', 'declare', 'decleration', 'define', 'defragmentation', 'degree', 'department', 'depth', 'dev', 'develop', 'developed', 'development', 'difference', 'different', 'differentiate', 'discoverer', 'disk', 'do', 'doe', 'dowload', 'download', 'driver', 'drop', 'duplicate', 'duration', 'dvd', 'dy', 'editor', 'education', 'effective', 'effectively', 'evening', 'exam', 'expensive', 'explain', 'expression', 'facility', 'faculty', 'fee', 'feedback', 'final', 'financial', 'first', 'for', 'fragmentation', 'freeze', 'freeze/unfreeze', 'from', 'function', 'gdb', 'get', 'give', 'go', 'goal', 'good', 'goodbye', 'government', 'gpa', 'grade', 'grading', 'group', 'guidlines', 'hand', 'handout', 'happens', 'have', 'held', 'hello', 'help', 'hey', 'hi', 'highly', 'history', 'hour', 'how', 'i', 'id', 'ide', 'idea', 'ideal', 'ides', 'if', 'importance', 'important', 'improve', 'in', 'influence', 'installment', 'institution', 'instructor', 'interesting', 'international', 'inventer', 'is', 'issue', 'it', 'k', 'karien', 'keise', 'ki', 'kind', 'know', 'language', 'later', 'latest', 'learn', 'learning', 'lectuer', 'lecture', 'library', 'life', 'link', 'list', 'liye', 'located', 'logical', 'look', 'manage', 'management', 'many', 'mark', 'may', 'me', 'mean', 'midterm', 'migration', 'minimum', 'mistake', 'morning', 'much', 'must', 'my', \"n't\", 'name', 'necessary', 'need', 'new', 'noc', 'not', 'objection', 'obtain', 'obtaining', 'of', 'offer', 'offered', 'on', 'one', 'online', 'oop', 'open', 'operate', 'operator', 'option', 'or', 'other', 'our', 'out', 'over', 'overseas', 'pakistan', 'pakistani', 'paper', 'passing', 'pay', 'paying', 'payment', 'performance', 'physical', 'pitfall', 'plagiarism', 'plagiarizing', 'platform', 'playable', 'please', 'plus', 'policy', 'possible', 'practical', 'preferable', 'private', 'pro', 'procedure', 'process', 'professor', 'program', 'programming', 'protocol', 'provide', 'province', 'purpose', 'quality', 'question', 'quiz', 'raise', 'reach', 'really', 'recheching', 'recipie', 'recognized', 'recommended', 'required', 'requirement', 's', 'same', 'saved', 'scheme', 'see', 'select', 'selection', 'selection/to', 'semester', 'service', 'session', 'should', 'significance', 'sir', 'skill', 'software', 'some', 'stand', 'standard', 'status', 'still', 'stored', 'structure', 'struggling', 'student', 'study', 'study/degree', 'studying', 'submission', 'support', 'syllabus', 'system', 'taking', 'taught', 'teacher', 'technical', 'technique', 'technologo', 'tell', 'term', 'thank', 'thanks', 'the', 'there', 'thing', 'this', 'through', 'time', 'tip', 'to', 'touch', 'tough', 'traditional', 'tuition', 'twain', 'type', 'unfreeze', 'uniform', 'university', 'unplayable', 'use', 'used', 'v', 'various', 'version', 'very', 'virtual', 'visit', 'vu', 'wa', 'want', 'watch', 'way', 'we', 'weightage', 'what', 'whats', 'whatsapp', 'whay', 'when', 'where', 'which', 'who', 'whom', 'why', 'will', 'window', 'with', 'without', 'work', 'working', 'writing', 'year', 'you', 'your']\n"
          ]
        }
      ],
      "source": [
        "# lemmaztize and lower each word and remove duplicates\n",
        "nltk.download('wordnet')\n",
        "words = [lemmatizer.lemmatize(w.lower()) for w in words if w not in ignore_words]\n",
        "words = sorted(list(set(words)))\n",
        "# sort classes\n",
        "classes = sorted(list(set(classes)))\n",
        "# documents = combination between patterns and intents\n",
        "print (len(documents), \"documents\")\n",
        "# classes = intents\n",
        "print (len(classes), \"classes\", classes)\n",
        "# words = all words, vocabulary\n",
        "print (len(words), \"unique lemmatized words\", words)\n",
        "pickle.dump(words,open('texts.pkl','wb'))\n",
        "pickle.dump(classes,open('labels.pkl','wb'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DZDzp9HIMs30",
        "outputId": "d7d6ee62-a5d5-438a-c974-dc7c679ec126"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating the Data for Our Model......\n",
            "Creating an List (Empty) for Output.....\n",
            "Creating Training Set, Bag of Words for our Model.....\n",
            "Training data created\n"
          ]
        }
      ],
      "source": [
        "print(\"Creating the Data for Our Model......\")\n",
        "training = []\n",
        "output = []\n",
        "print(\"Creating an List (Empty) for Output.....\")\n",
        "output_empty = [0] * len(classes)\n",
        "print(\"Creating Training Set, Bag of Words for our Model.....\")\n",
        "\n",
        "for document in documents:\n",
        "  #initialize our bag of words\n",
        "  bag = []\n",
        "  #list of tokenized words for the pattern\n",
        "  pattern_words = document[0]\n",
        "  #lemmatize each word - create base word, in attempt to represent related words\n",
        "  pattern_words = [lemmatizer.lemmatize(word.lower()) for word in pattern_words]\n",
        "  #create our bag of words array with 1, if word match found in current pattern\n",
        "  for word in words:\n",
        "    bag.append(1) if word in pattern_words else bag.append(0)\n",
        "\n",
        "    #output is a '0' for each tag and '1' for current tag (for each pattern)\n",
        "    output_row = list(output_empty)\n",
        "    output_row[classes.index(document[1])] = 1\n",
        "\n",
        "    training.append([bag, output_row])\n",
        "\n",
        "#shuffle our features into np.array\n",
        "random.shuffle(training)\n",
        "training = np.array(training, dtype=object)\n",
        "\n",
        "#create train and test lists. X - patterns, Y - intents\n",
        "train_x = list(training[:,0])\n",
        "train_y = list(training[:,1])\n",
        "\n",
        "print(\"Training data created\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iiA5mfO_bf5G"
      },
      "source": [
        "# **4-Model Training**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-gDV4j8l0L0"
      },
      "source": [
        "## **4.1-Neural Netowrk**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Em4aCw6ImU3R"
      },
      "source": [
        "### **4.1.1 Model Saving**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "id": "GwFda9VsmgQC",
        "outputId": "8d144a90-5205-48d3-c76c-95b0ea823a95"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-73dfaa568609>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'chat_model'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"model created\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ],
      "source": [
        "model.save('chat_model_1', hist)\n",
        "print(\"model created\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HvFngP6nzWA"
      },
      "source": [
        "## **4.2- LSTM**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1nVA9smHqt4v"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM,Dense,Flatten,Dropout, BatchNormalization\n",
        "from tensorflow.keras.layers import Bidirectional"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLxnK1_RLO4a"
      },
      "source": [
        "### **4.2.1 - Model Architecture 1**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kcFSuMKmq2C_",
        "outputId": "0e759fa7-fb42-4085-ba9d-c8d91f6c2105"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm (LSTM)                 (None, 391, 64)           16896     \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 391, 64)           0         \n",
            "                                                                 \n",
            " batch_normalization (Batch  (None, 391, 64)           256       \n",
            " Normalization)                                                  \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 391, 32)           12416     \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 391, 32)           0         \n",
            "                                                                 \n",
            " batch_normalization_1 (Bat  (None, 391, 32)           128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " lstm_2 (LSTM)               (None, 16)                3136      \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 16)                0         \n",
            "                                                                 \n",
            " batch_normalization_2 (Bat  (None, 16)                64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense (Dense)               (None, 113)               1921      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 34817 (136.00 KB)\n",
            "Trainable params: 34593 (135.13 KB)\n",
            "Non-trainable params: 224 (896.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/200\n",
            "932/932 [==============================] - 50s 44ms/step - loss: 3.7480 - accuracy: 0.1265\n",
            "Epoch 2/200\n",
            "932/932 [==============================] - 40s 43ms/step - loss: 2.4902 - accuracy: 0.2838\n",
            "Epoch 3/200\n",
            "932/932 [==============================] - 38s 41ms/step - loss: 1.9142 - accuracy: 0.4111\n",
            "Epoch 4/200\n",
            "932/932 [==============================] - 39s 42ms/step - loss: 1.5560 - accuracy: 0.4975\n",
            "Epoch 5/200\n",
            "932/932 [==============================] - 39s 42ms/step - loss: 1.2905 - accuracy: 0.5724\n",
            "Epoch 6/200\n",
            "932/932 [==============================] - 39s 42ms/step - loss: 1.1154 - accuracy: 0.6249\n",
            "Epoch 7/200\n",
            "932/932 [==============================] - 39s 42ms/step - loss: 0.9475 - accuracy: 0.6797\n",
            "Epoch 8/200\n",
            "932/932 [==============================] - 39s 42ms/step - loss: 0.7545 - accuracy: 0.7403\n",
            "Epoch 9/200\n",
            "932/932 [==============================] - 39s 42ms/step - loss: 2.5974 - accuracy: 0.4211\n",
            "Epoch 10/200\n",
            "932/932 [==============================] - 39s 41ms/step - loss: 2.5428 - accuracy: 0.2704\n",
            "Epoch 11/200\n",
            "932/932 [==============================] - 41s 44ms/step - loss: 1.8707 - accuracy: 0.4144\n",
            "Epoch 12/200\n",
            "932/932 [==============================] - 39s 42ms/step - loss: 1.4932 - accuracy: 0.5181\n",
            "Epoch 13/200\n",
            "932/932 [==============================] - 40s 43ms/step - loss: 1.2837 - accuracy: 0.5815\n",
            "Epoch 14/200\n",
            "932/932 [==============================] - 38s 41ms/step - loss: 1.9412 - accuracy: 0.5107\n",
            "Epoch 15/200\n",
            "932/932 [==============================] - 38s 41ms/step - loss: 4.6635 - accuracy: 0.0229\n",
            "Epoch 16/200\n",
            "932/932 [==============================] - 38s 41ms/step - loss: 4.6477 - accuracy: 0.0231\n",
            "Epoch 17/200\n",
            "932/932 [==============================] - 38s 41ms/step - loss: 4.6473 - accuracy: 0.0225\n",
            "Epoch 18/200\n",
            "932/932 [==============================] - 38s 40ms/step - loss: 4.6473 - accuracy: 0.0226\n",
            "Epoch 19/200\n",
            "932/932 [==============================] - 38s 40ms/step - loss: 4.6473 - accuracy: 0.0223\n",
            "Epoch 20/200\n",
            "932/932 [==============================] - 38s 40ms/step - loss: 4.6470 - accuracy: 0.0229\n",
            "Epoch 21/200\n",
            "932/932 [==============================] - 38s 41ms/step - loss: 4.6273 - accuracy: 0.0257\n",
            "Epoch 22/200\n",
            "932/932 [==============================] - 38s 41ms/step - loss: 4.2708 - accuracy: 0.0583\n",
            "Epoch 23/200\n",
            "932/932 [==============================] - 38s 41ms/step - loss: 4.0137 - accuracy: 0.0733\n",
            "Epoch 24/200\n",
            "932/932 [==============================] - 38s 41ms/step - loss: 3.8274 - accuracy: 0.0846\n",
            "Epoch 25/200\n",
            "932/932 [==============================] - 38s 41ms/step - loss: 3.6292 - accuracy: 0.1055\n",
            "Epoch 26/200\n",
            "932/932 [==============================] - 38s 40ms/step - loss: 3.3759 - accuracy: 0.1343\n",
            "Epoch 27/200\n",
            "932/932 [==============================] - 38s 41ms/step - loss: 3.2225 - accuracy: 0.1516\n",
            "Epoch 28/200\n",
            "932/932 [==============================] - 38s 41ms/step - loss: 2.8982 - accuracy: 0.2023\n",
            "Epoch 29/200\n",
            "932/932 [==============================] - 38s 41ms/step - loss: 2.5905 - accuracy: 0.2644\n",
            "Epoch 30/200\n",
            "932/932 [==============================] - 38s 40ms/step - loss: 2.2963 - accuracy: 0.3275\n",
            "Epoch 31/200\n",
            "932/932 [==============================] - 38s 40ms/step - loss: 2.1121 - accuracy: 0.3740\n",
            "Epoch 32/200\n",
            "932/932 [==============================] - 37s 40ms/step - loss: 1.9008 - accuracy: 0.4229\n",
            "Epoch 33/200\n",
            "932/932 [==============================] - 37s 40ms/step - loss: 1.7342 - accuracy: 0.4672\n",
            "Epoch 34/200\n",
            "932/932 [==============================] - 38s 41ms/step - loss: 1.9042 - accuracy: 0.4472\n",
            "Epoch 35/200\n",
            "932/932 [==============================] - 38s 40ms/step - loss: 1.7356 - accuracy: 0.4649\n",
            "Epoch 36/200\n",
            "932/932 [==============================] - 38s 41ms/step - loss: 1.8276 - accuracy: 0.4556\n",
            "Epoch 37/200\n",
            "932/932 [==============================] - 38s 40ms/step - loss: 1.4641 - accuracy: 0.5400\n",
            "Epoch 38/200\n",
            "932/932 [==============================] - 38s 40ms/step - loss: 1.2916 - accuracy: 0.5909\n",
            "Epoch 39/200\n",
            "932/932 [==============================] - 38s 41ms/step - loss: 1.1923 - accuracy: 0.6252\n",
            "Epoch 40/200\n",
            "932/932 [==============================] - 38s 40ms/step - loss: 1.1342 - accuracy: 0.6425\n",
            "Epoch 41/200\n",
            "932/932 [==============================] - 38s 41ms/step - loss: 1.0795 - accuracy: 0.6629\n",
            "Epoch 42/200\n",
            "932/932 [==============================] - 38s 41ms/step - loss: 1.4595 - accuracy: 0.5761\n",
            "Epoch 43/200\n",
            "932/932 [==============================] - 38s 41ms/step - loss: 1.1271 - accuracy: 0.6431\n",
            "Epoch 44/200\n",
            "932/932 [==============================] - 38s 41ms/step - loss: 0.9699 - accuracy: 0.6945\n",
            "Epoch 45/200\n",
            "932/932 [==============================] - 38s 40ms/step - loss: 0.8774 - accuracy: 0.7208\n",
            "Epoch 46/200\n",
            "932/932 [==============================] - 38s 41ms/step - loss: 0.8941 - accuracy: 0.7227\n",
            "Epoch 47/200\n",
            "932/932 [==============================] - 38s 41ms/step - loss: 0.7943 - accuracy: 0.7474\n",
            "Epoch 48/200\n",
            "932/932 [==============================] - 38s 41ms/step - loss: 0.6979 - accuracy: 0.7768\n",
            "Epoch 49/200\n",
            "932/932 [==============================] - 38s 41ms/step - loss: 1.5678 - accuracy: 0.5501\n",
            "Epoch 50/200\n",
            "932/932 [==============================] - 38s 40ms/step - loss: 0.9619 - accuracy: 0.6883\n",
            "Epoch 51/200\n",
            "290/932 [========>.....................] - ETA: 26s - loss: 0.8573 - accuracy: 0.7260"
          ]
        }
      ],
      "source": [
        "#Create the LSTM network\n",
        "model = Sequential()\n",
        "model.add(LSTM(64, input_shape=(len(train_x[0]),1), return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(BatchNormalization())\n",
        "model.add(LSTM(32,return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(BatchNormalization())\n",
        "model.add(LSTM(16,return_sequences=False))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(len(train_y[0]),activation=\"softmax\"))\n",
        "model.summary()\n",
        "# Compile model. Stochastic gradient descent with Nesterov accelerated gradient gives good results for this model\n",
        "lr_schedule = ExponentialDecay(\n",
        "    initial_learning_rate=0.01,\n",
        "    decay_steps=10000,\n",
        "    decay_rate=0.9)\n",
        "#adam = Adam(learning_rate=lr_schedule)\n",
        "sgd = SGD(learning_rate=0.01 ,momentum=0.9, nesterov=True)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "#fitting and saving the model\n",
        "hist = model.fit(np.array(train_x), np.array(train_y), epochs=200, batch_size=128, verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qQz12ssLy1C"
      },
      "source": [
        "### **4.2.2 - Model Architecture 2**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pUkU6STQLw-S",
        "outputId": "cca37508-c9ed-4cd6-de99-37c3d6f31b50"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_4 (LSTM)               (None, 391, 128)          66560     \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 391, 128)          0         \n",
            "                                                                 \n",
            " lstm_5 (LSTM)               (None, 391, 64)           49408     \n",
            "                                                                 \n",
            " dropout_5 (Dropout)         (None, 391, 64)           0         \n",
            "                                                                 \n",
            " batch_normalization_3 (Bat  (None, 391, 64)           256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " lstm_6 (LSTM)               (None, 391, 32)           12416     \n",
            "                                                                 \n",
            " dropout_6 (Dropout)         (None, 391, 32)           0         \n",
            "                                                                 \n",
            " batch_normalization_4 (Bat  (None, 391, 32)           128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " lstm_7 (LSTM)               (None, 16)                3136      \n",
            "                                                                 \n",
            " dropout_7 (Dropout)         (None, 16)                0         \n",
            "                                                                 \n",
            " batch_normalization_5 (Bat  (None, 16)                64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 113)               1921      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 133889 (523.00 KB)\n",
            "Trainable params: 133665 (522.13 KB)\n",
            "Non-trainable params: 224 (896.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "932/932 [==============================] - 74s 65ms/step - loss: 4.5581 - accuracy: 0.0383\n",
            "Epoch 2/100\n",
            "932/932 [==============================] - 60s 65ms/step - loss: 4.1713 - accuracy: 0.0804\n",
            "Epoch 3/100\n",
            "932/932 [==============================] - 60s 65ms/step - loss: 3.8606 - accuracy: 0.1112\n",
            "Epoch 4/100\n",
            "932/932 [==============================] - 61s 65ms/step - loss: 3.5848 - accuracy: 0.1402\n",
            "Epoch 5/100\n",
            "932/932 [==============================] - 60s 65ms/step - loss: 3.3244 - accuracy: 0.1727\n",
            "Epoch 6/100\n",
            "932/932 [==============================] - 61s 65ms/step - loss: 3.0648 - accuracy: 0.2091\n",
            "Epoch 7/100\n",
            "932/932 [==============================] - 61s 65ms/step - loss: 2.8126 - accuracy: 0.2503\n",
            "Epoch 8/100\n",
            "932/932 [==============================] - 60s 65ms/step - loss: 2.5778 - accuracy: 0.2959\n",
            "Epoch 9/100\n",
            "932/932 [==============================] - 61s 65ms/step - loss: 2.3691 - accuracy: 0.3390\n",
            "Epoch 10/100\n",
            "932/932 [==============================] - 61s 66ms/step - loss: 2.1946 - accuracy: 0.3766\n",
            "Epoch 11/100\n",
            "932/932 [==============================] - 60s 65ms/step - loss: 2.0436 - accuracy: 0.4100\n",
            "Epoch 12/100\n",
            "932/932 [==============================] - 61s 65ms/step - loss: 1.9066 - accuracy: 0.4381\n",
            "Epoch 13/100\n",
            "932/932 [==============================] - 61s 65ms/step - loss: 1.7714 - accuracy: 0.4702\n",
            "Epoch 14/100\n",
            "932/932 [==============================] - 60s 65ms/step - loss: 1.6573 - accuracy: 0.4969\n",
            "Epoch 15/100\n",
            "932/932 [==============================] - 61s 66ms/step - loss: 1.5487 - accuracy: 0.5222\n",
            "Epoch 16/100\n",
            "932/932 [==============================] - 61s 65ms/step - loss: 1.4633 - accuracy: 0.5440\n",
            "Epoch 17/100\n",
            "932/932 [==============================] - 61s 65ms/step - loss: 1.3791 - accuracy: 0.5649\n",
            "Epoch 18/100\n",
            "932/932 [==============================] - 61s 66ms/step - loss: 1.3058 - accuracy: 0.5831\n",
            "Epoch 19/100\n",
            "932/932 [==============================] - 61s 65ms/step - loss: 1.2500 - accuracy: 0.5993\n",
            "Epoch 20/100\n",
            "932/932 [==============================] - 61s 65ms/step - loss: 1.1978 - accuracy: 0.6140\n",
            "Epoch 21/100\n",
            "932/932 [==============================] - 61s 66ms/step - loss: 1.1476 - accuracy: 0.6270\n",
            "Epoch 22/100\n",
            "932/932 [==============================] - 61s 65ms/step - loss: 1.1053 - accuracy: 0.6398\n",
            "Epoch 23/100\n",
            "932/932 [==============================] - 60s 65ms/step - loss: 1.0539 - accuracy: 0.6572\n",
            "Epoch 24/100\n",
            "932/932 [==============================] - 61s 65ms/step - loss: 1.0007 - accuracy: 0.6712\n",
            "Epoch 25/100\n",
            "932/932 [==============================] - 61s 65ms/step - loss: 0.9506 - accuracy: 0.6858\n",
            "Epoch 26/100\n",
            "932/932 [==============================] - 61s 65ms/step - loss: 0.8948 - accuracy: 0.7020\n",
            "Epoch 27/100\n",
            "932/932 [==============================] - 61s 66ms/step - loss: 0.8563 - accuracy: 0.7137\n",
            "Epoch 28/100\n",
            "932/932 [==============================] - 61s 65ms/step - loss: 0.8044 - accuracy: 0.7297\n",
            "Epoch 29/100\n",
            "932/932 [==============================] - 61s 65ms/step - loss: 0.7589 - accuracy: 0.7424\n",
            "Epoch 30/100\n",
            "932/932 [==============================] - 61s 65ms/step - loss: 0.7170 - accuracy: 0.7551\n",
            "Epoch 31/100\n",
            "932/932 [==============================] - 61s 65ms/step - loss: 0.6875 - accuracy: 0.7639\n",
            "Epoch 32/100\n",
            "932/932 [==============================] - 61s 66ms/step - loss: 0.6603 - accuracy: 0.7741\n",
            "Epoch 33/100\n",
            "932/932 [==============================] - 61s 65ms/step - loss: 0.6073 - accuracy: 0.7915\n",
            "Epoch 34/100\n",
            "932/932 [==============================] - 61s 65ms/step - loss: 0.5659 - accuracy: 0.8057\n",
            "Epoch 35/100\n",
            "932/932 [==============================] - 61s 66ms/step - loss: 0.6011 - accuracy: 0.7981\n",
            "Epoch 36/100\n",
            "932/932 [==============================] - 61s 65ms/step - loss: 0.5179 - accuracy: 0.8205\n",
            "Epoch 37/100\n",
            "932/932 [==============================] - 61s 65ms/step - loss: 0.4980 - accuracy: 0.8292\n",
            "Epoch 38/100\n",
            "932/932 [==============================] - 61s 66ms/step - loss: 0.4457 - accuracy: 0.8473\n",
            "Epoch 39/100\n",
            "932/932 [==============================] - 61s 65ms/step - loss: 0.4617 - accuracy: 0.8452\n",
            "Epoch 40/100\n",
            "932/932 [==============================] - 61s 66ms/step - loss: 0.3936 - accuracy: 0.8670\n",
            "Epoch 41/100\n",
            "932/932 [==============================] - 61s 66ms/step - loss: 0.3909 - accuracy: 0.8679\n",
            "Epoch 42/100\n",
            "932/932 [==============================] - 61s 65ms/step - loss: 0.4034 - accuracy: 0.8678\n",
            "Epoch 43/100\n",
            "932/932 [==============================] - 61s 66ms/step - loss: 0.3335 - accuracy: 0.8880\n",
            "Epoch 44/100\n",
            "932/932 [==============================] - 62s 66ms/step - loss: 0.5814 - accuracy: 0.8340\n",
            "Epoch 45/100\n",
            "932/932 [==============================] - 61s 65ms/step - loss: 0.3660 - accuracy: 0.8764\n",
            "Epoch 46/100\n",
            "932/932 [==============================] - 61s 66ms/step - loss: 0.2975 - accuracy: 0.9026\n",
            "Epoch 47/100\n",
            "932/932 [==============================] - 61s 65ms/step - loss: 0.3078 - accuracy: 0.9026\n",
            "Epoch 48/100\n",
            "932/932 [==============================] - 61s 65ms/step - loss: 0.2302 - accuracy: 0.9287\n",
            "Epoch 49/100\n",
            "932/932 [==============================] - 61s 65ms/step - loss: 0.2394 - accuracy: 0.9268\n",
            "Epoch 50/100\n",
            "932/932 [==============================] - 61s 65ms/step - loss: 0.1978 - accuracy: 0.9401\n",
            "Epoch 51/100\n",
            "932/932 [==============================] - 61s 65ms/step - loss: 0.1907 - accuracy: 0.9419\n",
            "Epoch 52/100\n",
            "932/932 [==============================] - 61s 65ms/step - loss: 0.1922 - accuracy: 0.9428\n",
            "Epoch 53/100\n",
            "932/932 [==============================] - 60s 65ms/step - loss: 0.1581 - accuracy: 0.9544\n",
            "Epoch 54/100\n",
            "932/932 [==============================] - 61s 66ms/step - loss: 0.1979 - accuracy: 0.9424\n",
            "Epoch 55/100\n",
            "932/932 [==============================] - 61s 65ms/step - loss: 0.1646 - accuracy: 0.9520\n",
            "Epoch 56/100\n",
            "932/932 [==============================] - 61s 65ms/step - loss: 0.1581 - accuracy: 0.9541\n",
            "Epoch 57/100\n",
            "932/932 [==============================] - 61s 66ms/step - loss: 0.1539 - accuracy: 0.9551\n",
            "Epoch 58/100\n",
            "932/932 [==============================] - 61s 66ms/step - loss: 0.3186 - accuracy: 0.9161\n",
            "Epoch 59/100\n",
            "932/932 [==============================] - 61s 65ms/step - loss: 0.1247 - accuracy: 0.9639\n",
            "Epoch 60/100\n",
            "932/932 [==============================] - 61s 66ms/step - loss: 0.1370 - accuracy: 0.9608\n",
            "Epoch 61/100\n",
            "932/932 [==============================] - 60s 65ms/step - loss: 0.1208 - accuracy: 0.9656\n",
            "Epoch 62/100\n",
            "932/932 [==============================] - 61s 65ms/step - loss: 0.1107 - accuracy: 0.9679\n",
            "Epoch 63/100\n",
            "932/932 [==============================] - 62s 66ms/step - loss: 0.1003 - accuracy: 0.9710\n",
            "Epoch 64/100\n",
            "932/932 [==============================] - 61s 65ms/step - loss: 0.1150 - accuracy: 0.9669\n",
            "Epoch 65/100\n",
            "932/932 [==============================] - 61s 65ms/step - loss: 0.0971 - accuracy: 0.9718\n",
            "Epoch 66/100\n",
            "932/932 [==============================] - 61s 65ms/step - loss: 0.1128 - accuracy: 0.9679\n",
            "Epoch 67/100\n",
            "932/932 [==============================] - 61s 65ms/step - loss: 0.0868 - accuracy: 0.9747\n",
            "Epoch 68/100\n",
            "932/932 [==============================] - 62s 66ms/step - loss: 0.0943 - accuracy: 0.9726\n",
            "Epoch 69/100\n",
            "932/932 [==============================] - 61s 65ms/step - loss: 0.0844 - accuracy: 0.9757\n",
            "Epoch 70/100\n",
            "932/932 [==============================] - 61s 65ms/step - loss: 0.1311 - accuracy: 0.9628\n",
            "Epoch 71/100\n",
            "932/932 [==============================] - 61s 66ms/step - loss: 0.0825 - accuracy: 0.9755\n",
            "Epoch 72/100\n",
            "932/932 [==============================] - 61s 65ms/step - loss: 0.0786 - accuracy: 0.9768\n",
            "Epoch 73/100\n",
            "932/932 [==============================] - 61s 65ms/step - loss: 0.0759 - accuracy: 0.9776\n",
            "Epoch 74/100\n",
            "932/932 [==============================] - 61s 65ms/step - loss: 1.2304 - accuracy: 0.7290\n",
            "Epoch 75/100\n",
            "932/932 [==============================] - 61s 65ms/step - loss: 0.1979 - accuracy: 0.9368\n",
            "Epoch 76/100\n",
            "932/932 [==============================] - 61s 65ms/step - loss: 0.1205 - accuracy: 0.9648\n",
            "Epoch 77/100\n",
            "932/932 [==============================] - 61s 65ms/step - loss: 0.0978 - accuracy: 0.9722\n",
            "Epoch 78/100\n",
            "932/932 [==============================] - 61s 65ms/step - loss: 0.0864 - accuracy: 0.9756\n",
            "Epoch 79/100\n",
            "932/932 [==============================] - 61s 65ms/step - loss: 0.1165 - accuracy: 0.9685\n",
            "Epoch 80/100\n",
            "932/932 [==============================] - 60s 65ms/step - loss: 0.0837 - accuracy: 0.9762\n",
            "Epoch 81/100\n",
            "932/932 [==============================] - 61s 65ms/step - loss: 0.0764 - accuracy: 0.9782\n",
            "Epoch 82/100\n",
            "932/932 [==============================] - 61s 66ms/step - loss: 0.0731 - accuracy: 0.9790\n",
            "Epoch 83/100\n",
            "932/932 [==============================] - 61s 66ms/step - loss: 0.1561 - accuracy: 0.9605\n",
            "Epoch 84/100\n",
            "932/932 [==============================] - 61s 66ms/step - loss: 0.0777 - accuracy: 0.9776\n",
            "Epoch 85/100\n",
            "932/932 [==============================] - 61s 65ms/step - loss: 0.0720 - accuracy: 0.9793\n",
            "Epoch 86/100\n",
            "932/932 [==============================] - 61s 66ms/step - loss: 0.0667 - accuracy: 0.9808\n",
            "Epoch 87/100\n",
            "932/932 [==============================] - 61s 66ms/step - loss: 0.0652 - accuracy: 0.9810\n",
            "Epoch 88/100\n",
            "932/932 [==============================] - 61s 65ms/step - loss: 0.0694 - accuracy: 0.9800\n",
            "Epoch 89/100\n",
            "932/932 [==============================] - 61s 65ms/step - loss: 0.0655 - accuracy: 0.9807\n",
            "Epoch 90/100\n",
            "932/932 [==============================] - 62s 66ms/step - loss: 0.0578 - accuracy: 0.9832\n",
            "Epoch 91/100\n",
            "932/932 [==============================] - 61s 65ms/step - loss: 0.0579 - accuracy: 0.9834\n",
            "Epoch 92/100\n",
            "932/932 [==============================] - 61s 66ms/step - loss: 0.0619 - accuracy: 0.9817\n",
            "Epoch 93/100\n",
            "932/932 [==============================] - 62s 66ms/step - loss: 0.0569 - accuracy: 0.9832\n",
            "Epoch 94/100\n",
            "932/932 [==============================] - 61s 66ms/step - loss: 0.0571 - accuracy: 0.9830\n",
            "Epoch 95/100\n",
            "932/932 [==============================] - 61s 65ms/step - loss: 0.0546 - accuracy: 0.9834\n",
            "Epoch 96/100\n",
            "932/932 [==============================] - 61s 66ms/step - loss: 0.0576 - accuracy: 0.9827\n",
            "Epoch 97/100\n",
            "932/932 [==============================] - 61s 65ms/step - loss: 0.0519 - accuracy: 0.9849\n",
            "Epoch 98/100\n",
            "932/932 [==============================] - 61s 65ms/step - loss: 0.0682 - accuracy: 0.9800\n",
            "Epoch 99/100\n",
            "932/932 [==============================] - 61s 65ms/step - loss: 0.0513 - accuracy: 0.9848\n",
            "Epoch 100/100\n",
            "932/932 [==============================] - 61s 65ms/step - loss: 0.0506 - accuracy: 0.9848\n"
          ]
        }
      ],
      "source": [
        "#Create the LSTM network\n",
        "model = Sequential()\n",
        "model.add(LSTM(128, input_shape=(len(train_x[0]),1), return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(64,return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(BatchNormalization())\n",
        "model.add(LSTM(32,return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(BatchNormalization())\n",
        "model.add(LSTM(16,return_sequences=False))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(len(train_y[0]),activation=\"softmax\"))\n",
        "model.summary()\n",
        "# Compile model. Stochastic gradient descent with Nesterov accelerated gradient gives good results for this model\n",
        "lr_schedule = ExponentialDecay(\n",
        "    initial_learning_rate=0.01,\n",
        "    decay_steps=10000,\n",
        "    decay_rate=0.9)\n",
        "#adam = Adam(learning_rate=lr_schedule)\n",
        "sgd = SGD(learning_rate=0.001 ,momentum=0.9, nesterov=True)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "#fitting and saving the model\n",
        "hist = model.fit(np.array(train_x), np.array(train_y), epochs=100, batch_size=128, verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGcC29CeWi4y"
      },
      "source": [
        "### 4.2.2.1 - Model Saving"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ueVFk8ZWYS4s",
        "outputId": "d92c59a1-2a4b-4884-93f2-7df4b23eb575"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model created and saved\n"
          ]
        }
      ],
      "source": [
        "model.save( path_to_save_model+'chat_model_2', hist)\n",
        "print(\"model created and saved\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezj7AyRaQ1Kl"
      },
      "source": [
        "### **4.2.3 - Model Architecture 3**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6cJqEH_qWb-8",
        "outputId": "21242e2e-0468-4289-b301-1f70a88fe9ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " bidirectional (Bidirection  (None, 391, 128)          33792     \n",
            " al)                                                             \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 391, 128)          0         \n",
            "                                                                 \n",
            " bidirectional_1 (Bidirecti  (None, 391, 64)           41216     \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 391, 64)           0         \n",
            "                                                                 \n",
            " batch_normalization (Batch  (None, 391, 64)           256       \n",
            " Normalization)                                                  \n",
            "                                                                 \n",
            " bidirectional_2 (Bidirecti  (None, 32)                10368     \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 32)                0         \n",
            "                                                                 \n",
            " batch_normalization_1 (Bat  (None, 32)                128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense (Dense)               (None, 113)               3729      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 89489 (349.57 KB)\n",
            "Trainable params: 89297 (348.82 KB)\n",
            "Non-trainable params: 192 (768.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = Sequential()\n",
        "model.add(Bidirectional(LSTM(units=64,input_shape=(len(train_x[0]),1),return_sequences=True)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Bidirectional(LSTM(units=32,return_sequences=True)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Bidirectional(LSTM(units=16,return_sequences=False)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(len(train_y[0]),activation=\"softmax\"))\n",
        "\n",
        "# Build the model\n",
        "model.build(input_shape=(None, len(train_x[0]), 1))\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57ZFEBvntTGt",
        "outputId": "f96deecc-a620-4aa9-f7a7-a27bada9adf5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "932/932 [==============================] - 89s 80ms/step - loss: 4.5237 - accuracy: 0.0520\n",
            "Epoch 2/100\n",
            "932/932 [==============================] - 73s 79ms/step - loss: 3.7492 - accuracy: 0.1474\n",
            "Epoch 3/100\n",
            "932/932 [==============================] - 74s 79ms/step - loss: 3.2352 - accuracy: 0.2316\n",
            "Epoch 4/100\n",
            "932/932 [==============================] - 73s 79ms/step - loss: 2.7851 - accuracy: 0.3036\n",
            "Epoch 5/100\n",
            "932/932 [==============================] - 73s 79ms/step - loss: 2.4090 - accuracy: 0.3725\n",
            "Epoch 6/100\n",
            "932/932 [==============================] - 73s 78ms/step - loss: 2.0772 - accuracy: 0.4478\n",
            "Epoch 7/100\n",
            "932/932 [==============================] - 74s 79ms/step - loss: 1.7939 - accuracy: 0.5148\n",
            "Epoch 8/100\n",
            "932/932 [==============================] - 74s 80ms/step - loss: 1.5713 - accuracy: 0.5678\n",
            "Epoch 9/100\n",
            "932/932 [==============================] - 74s 80ms/step - loss: 1.3666 - accuracy: 0.6160\n",
            "Epoch 10/100\n",
            "932/932 [==============================] - 74s 80ms/step - loss: 1.2013 - accuracy: 0.6577\n",
            "Epoch 11/100\n",
            "932/932 [==============================] - 74s 79ms/step - loss: 1.1264 - accuracy: 0.6824\n",
            "Epoch 12/100\n",
            "932/932 [==============================] - 74s 79ms/step - loss: 1.2004 - accuracy: 0.6516\n",
            "Epoch 13/100\n",
            "932/932 [==============================] - 74s 79ms/step - loss: 0.8443 - accuracy: 0.7513\n",
            "Epoch 14/100\n",
            "932/932 [==============================] - 73s 78ms/step - loss: 0.7252 - accuracy: 0.7837\n",
            "Epoch 15/100\n",
            "932/932 [==============================] - 73s 78ms/step - loss: 0.8774 - accuracy: 0.7444\n",
            "Epoch 16/100\n",
            "932/932 [==============================] - 73s 78ms/step - loss: 0.6226 - accuracy: 0.8124\n",
            "Epoch 17/100\n",
            "932/932 [==============================] - 74s 79ms/step - loss: 0.5666 - accuracy: 0.8262\n",
            "Epoch 18/100\n",
            "932/932 [==============================] - 73s 78ms/step - loss: 0.5038 - accuracy: 0.8444\n",
            "Epoch 19/100\n",
            "932/932 [==============================] - 73s 79ms/step - loss: 0.4577 - accuracy: 0.8591\n",
            "Epoch 20/100\n",
            "932/932 [==============================] - 73s 79ms/step - loss: 0.4877 - accuracy: 0.8506\n",
            "Epoch 21/100\n",
            "932/932 [==============================] - 74s 79ms/step - loss: 0.3760 - accuracy: 0.8852\n",
            "Epoch 22/100\n",
            "932/932 [==============================] - 73s 79ms/step - loss: 0.7034 - accuracy: 0.8027\n",
            "Epoch 23/100\n",
            "932/932 [==============================] - 73s 78ms/step - loss: 0.4057 - accuracy: 0.8716\n",
            "Epoch 24/100\n",
            "932/932 [==============================] - 74s 79ms/step - loss: 0.3222 - accuracy: 0.9016\n",
            "Epoch 25/100\n",
            "932/932 [==============================] - 74s 79ms/step - loss: 0.2788 - accuracy: 0.9162\n",
            "Epoch 26/100\n",
            "932/932 [==============================] - 74s 80ms/step - loss: 0.2577 - accuracy: 0.9236\n",
            "Epoch 27/100\n",
            "932/932 [==============================] - 75s 80ms/step - loss: 0.2141 - accuracy: 0.9397\n",
            "Epoch 28/100\n",
            "932/932 [==============================] - 75s 80ms/step - loss: 0.1942 - accuracy: 0.9459\n",
            "Epoch 29/100\n",
            "932/932 [==============================] - 75s 80ms/step - loss: 0.1748 - accuracy: 0.9508\n",
            "Epoch 30/100\n",
            "932/932 [==============================] - 75s 80ms/step - loss: 0.1582 - accuracy: 0.9571\n",
            "Epoch 31/100\n",
            "932/932 [==============================] - 74s 80ms/step - loss: 0.1455 - accuracy: 0.9603\n",
            "Epoch 32/100\n",
            "932/932 [==============================] - 74s 79ms/step - loss: 0.1420 - accuracy: 0.9614\n",
            "Epoch 33/100\n",
            "932/932 [==============================] - 75s 80ms/step - loss: 0.1262 - accuracy: 0.9656\n",
            "Epoch 34/100\n",
            "932/932 [==============================] - 74s 80ms/step - loss: 0.1160 - accuracy: 0.9687\n",
            "Epoch 35/100\n",
            "932/932 [==============================] - 74s 80ms/step - loss: 0.1170 - accuracy: 0.9677\n",
            "Epoch 36/100\n",
            "932/932 [==============================] - 75s 80ms/step - loss: 0.1075 - accuracy: 0.9699\n",
            "Epoch 37/100\n",
            "932/932 [==============================] - 75s 80ms/step - loss: 0.0997 - accuracy: 0.9726\n",
            "Epoch 38/100\n",
            "932/932 [==============================] - 75s 80ms/step - loss: 0.0975 - accuracy: 0.9725\n",
            "Epoch 39/100\n",
            "932/932 [==============================] - 74s 80ms/step - loss: 0.0927 - accuracy: 0.9743\n",
            "Epoch 40/100\n",
            "932/932 [==============================] - 74s 80ms/step - loss: 0.0884 - accuracy: 0.9760\n",
            "Epoch 41/100\n",
            "932/932 [==============================] - 75s 80ms/step - loss: 0.1597 - accuracy: 0.9528\n",
            "Epoch 42/100\n",
            "932/932 [==============================] - 74s 79ms/step - loss: 0.0862 - accuracy: 0.9760\n",
            "Epoch 43/100\n",
            "932/932 [==============================] - 75s 80ms/step - loss: 0.0803 - accuracy: 0.9778\n",
            "Epoch 44/100\n",
            "932/932 [==============================] - 75s 80ms/step - loss: 0.0789 - accuracy: 0.9787\n",
            "Epoch 45/100\n",
            "932/932 [==============================] - 74s 80ms/step - loss: 0.0696 - accuracy: 0.9814\n",
            "Epoch 46/100\n",
            "932/932 [==============================] - 74s 80ms/step - loss: 0.1011 - accuracy: 0.9714\n",
            "Epoch 47/100\n",
            "932/932 [==============================] - 74s 80ms/step - loss: 0.0888 - accuracy: 0.9745\n",
            "Epoch 48/100\n",
            "932/932 [==============================] - 74s 79ms/step - loss: 0.0687 - accuracy: 0.9809\n",
            "Epoch 49/100\n",
            "932/932 [==============================] - 75s 81ms/step - loss: 0.0600 - accuracy: 0.9840\n",
            "Epoch 50/100\n",
            "932/932 [==============================] - 75s 80ms/step - loss: 0.0578 - accuracy: 0.9844\n",
            "Epoch 51/100\n",
            "932/932 [==============================] - 74s 80ms/step - loss: 0.0562 - accuracy: 0.9844\n",
            "Epoch 52/100\n",
            "932/932 [==============================] - 75s 80ms/step - loss: 0.0529 - accuracy: 0.9862\n",
            "Epoch 53/100\n",
            "932/932 [==============================] - 75s 80ms/step - loss: 0.0539 - accuracy: 0.9853\n",
            "Epoch 54/100\n",
            "932/932 [==============================] - 75s 80ms/step - loss: 0.0554 - accuracy: 0.9852\n",
            "Epoch 55/100\n",
            "932/932 [==============================] - 76s 81ms/step - loss: 0.0486 - accuracy: 0.9873\n",
            "Epoch 56/100\n",
            "932/932 [==============================] - 75s 81ms/step - loss: 0.0555 - accuracy: 0.9848\n",
            "Epoch 57/100\n",
            "932/932 [==============================] - 76s 82ms/step - loss: 0.0472 - accuracy: 0.9875\n",
            "Epoch 58/100\n",
            "932/932 [==============================] - 76s 81ms/step - loss: 0.0434 - accuracy: 0.9885\n",
            "Epoch 59/100\n",
            "932/932 [==============================] - 76s 81ms/step - loss: 0.0437 - accuracy: 0.9883\n",
            "Epoch 60/100\n",
            "932/932 [==============================] - 77s 83ms/step - loss: 0.0482 - accuracy: 0.9866\n",
            "Epoch 61/100\n",
            "932/932 [==============================] - 76s 81ms/step - loss: 1.8266 - accuracy: 0.6585\n",
            "Epoch 62/100\n",
            "932/932 [==============================] - 76s 82ms/step - loss: 0.6956 - accuracy: 0.7751\n",
            "Epoch 63/100\n",
            "932/932 [==============================] - 76s 82ms/step - loss: 0.3384 - accuracy: 0.8836\n",
            "Epoch 64/100\n",
            "932/932 [==============================] - 76s 81ms/step - loss: 0.2430 - accuracy: 0.9169\n",
            "Epoch 65/100\n",
            "932/932 [==============================] - 76s 82ms/step - loss: 0.1965 - accuracy: 0.9355\n",
            "Epoch 66/100\n",
            "932/932 [==============================] - 76s 81ms/step - loss: 0.1629 - accuracy: 0.9475\n",
            "Epoch 67/100\n",
            "932/932 [==============================] - 76s 81ms/step - loss: 0.1416 - accuracy: 0.9549\n",
            "Epoch 68/100\n",
            "932/932 [==============================] - 76s 81ms/step - loss: 0.1350 - accuracy: 0.9579\n",
            "Epoch 69/100\n",
            "932/932 [==============================] - 75s 81ms/step - loss: 0.1189 - accuracy: 0.9626\n",
            "Epoch 70/100\n",
            "932/932 [==============================] - 76s 82ms/step - loss: 0.1042 - accuracy: 0.9680\n",
            "Epoch 71/100\n",
            "932/932 [==============================] - 76s 81ms/step - loss: 0.9430 - accuracy: 0.7589\n",
            "Epoch 72/100\n",
            "932/932 [==============================] - 76s 81ms/step - loss: 0.2478 - accuracy: 0.9144\n",
            "Epoch 73/100\n",
            "932/932 [==============================] - 76s 81ms/step - loss: 0.1939 - accuracy: 0.9343\n",
            "Epoch 74/100\n",
            "932/932 [==============================] - 75s 81ms/step - loss: 0.1517 - accuracy: 0.9503\n",
            "Epoch 75/100\n",
            "932/932 [==============================] - 76s 81ms/step - loss: 0.1292 - accuracy: 0.9583\n",
            "Epoch 76/100\n",
            "932/932 [==============================] - 76s 81ms/step - loss: 0.1177 - accuracy: 0.9625\n",
            "Epoch 77/100\n",
            "932/932 [==============================] - 75s 80ms/step - loss: 0.1046 - accuracy: 0.9674\n",
            "Epoch 78/100\n",
            "932/932 [==============================] - 76s 81ms/step - loss: 0.1538 - accuracy: 0.9545\n",
            "Epoch 79/100\n",
            "932/932 [==============================] - 75s 81ms/step - loss: 0.0958 - accuracy: 0.9706\n",
            "Epoch 80/100\n",
            "932/932 [==============================] - 76s 81ms/step - loss: 0.0812 - accuracy: 0.9749\n",
            "Epoch 81/100\n",
            "932/932 [==============================] - 75s 81ms/step - loss: 0.0730 - accuracy: 0.9781\n",
            "Epoch 82/100\n",
            "932/932 [==============================] - 75s 81ms/step - loss: 0.0717 - accuracy: 0.9784\n",
            "Epoch 83/100\n",
            "932/932 [==============================] - 76s 81ms/step - loss: 0.0665 - accuracy: 0.9798\n",
            "Epoch 84/100\n",
            "932/932 [==============================] - 75s 80ms/step - loss: 0.0638 - accuracy: 0.9807\n",
            "Epoch 85/100\n",
            "932/932 [==============================] - 75s 81ms/step - loss: 0.0615 - accuracy: 0.9810\n",
            "Epoch 86/100\n",
            "932/932 [==============================] - 75s 81ms/step - loss: 0.0605 - accuracy: 0.9812\n",
            "Epoch 87/100\n",
            "932/932 [==============================] - 75s 80ms/step - loss: 0.0541 - accuracy: 0.9832\n",
            "Epoch 88/100\n",
            "932/932 [==============================] - 76s 81ms/step - loss: 0.0606 - accuracy: 0.9812\n",
            "Epoch 89/100\n",
            "932/932 [==============================] - 75s 80ms/step - loss: 0.0531 - accuracy: 0.9838\n",
            "Epoch 90/100\n",
            "932/932 [==============================] - 75s 81ms/step - loss: 0.0498 - accuracy: 0.9844\n",
            "Epoch 91/100\n",
            "932/932 [==============================] - 75s 81ms/step - loss: 0.0480 - accuracy: 0.9844\n",
            "Epoch 92/100\n",
            "932/932 [==============================] - 75s 80ms/step - loss: 0.0460 - accuracy: 0.9856\n",
            "Epoch 93/100\n",
            "932/932 [==============================] - 75s 81ms/step - loss: 0.0445 - accuracy: 0.9858\n",
            "Epoch 94/100\n",
            "932/932 [==============================] - 75s 80ms/step - loss: 0.0455 - accuracy: 0.9854\n",
            "Epoch 95/100\n",
            "932/932 [==============================] - 75s 80ms/step - loss: 0.0434 - accuracy: 0.9862\n",
            "Epoch 96/100\n",
            "932/932 [==============================] - 76s 81ms/step - loss: 0.0416 - accuracy: 0.9865\n",
            "Epoch 97/100\n",
            "932/932 [==============================] - 76s 81ms/step - loss: 0.0432 - accuracy: 0.9860\n",
            "Epoch 98/100\n",
            "932/932 [==============================] - 76s 82ms/step - loss: 0.0406 - accuracy: 0.9869\n",
            "Epoch 99/100\n",
            "932/932 [==============================] - 76s 81ms/step - loss: 0.0404 - accuracy: 0.9867\n",
            "Epoch 100/100\n",
            "932/932 [==============================] - 76s 81ms/step - loss: 0.0397 - accuracy: 0.9872\n"
          ]
        }
      ],
      "source": [
        "sgd = SGD(learning_rate=0.001 ,momentum=0.9, nesterov=True)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "\n",
        "#fitting and saving the model\n",
        "hist = model.fit(np.array(train_x), np.array(train_y), epochs=100, batch_size=128, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tiXTxc8XNsYE",
        "outputId": "6974d4b2-d88e-41c2-d2d1-d55be208fc22"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model created and saved\n"
          ]
        }
      ],
      "source": [
        "model.save( path_to_save_model+'chat_model_3', hist)\n",
        "print(\"model created and saved\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQWlRcZXEmuq"
      },
      "source": [
        "### **4.2.4 - Model Architecture 4**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vIS8x11cEzeT",
        "outputId": "d088e33b-d89c-4935-95a9-c3f42273b025"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " bidirectional (Bidirection  (None, 391, 256)          133120    \n",
            " al)                                                             \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 391, 256)          0         \n",
            "                                                                 \n",
            " bidirectional_1 (Bidirecti  (None, 391, 128)          164352    \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 391, 128)          0         \n",
            "                                                                 \n",
            " batch_normalization (Batch  (None, 391, 128)          512       \n",
            " Normalization)                                                  \n",
            "                                                                 \n",
            " bidirectional_2 (Bidirecti  (None, 391, 64)           41216     \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 391, 64)           0         \n",
            "                                                                 \n",
            " batch_normalization_1 (Bat  (None, 391, 64)           256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " bidirectional_3 (Bidirecti  (None, 32)                10368     \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 32)                0         \n",
            "                                                                 \n",
            " batch_normalization_2 (Bat  (None, 32)                128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense (Dense)               (None, 113)               3729      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 353681 (1.35 MB)\n",
            "Trainable params: 353233 (1.35 MB)\n",
            "Non-trainable params: 448 (1.75 KB)\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "932/932 [==============================] - 141s 132ms/step - loss: 4.6924 - accuracy: 0.0300\n",
            "Epoch 2/100\n",
            "932/932 [==============================] - 125s 134ms/step - loss: 4.0253 - accuracy: 0.1153\n",
            "Epoch 3/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 3.4509 - accuracy: 0.2022\n",
            "Epoch 4/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 2.9579 - accuracy: 0.2890\n",
            "Epoch 5/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 2.5412 - accuracy: 0.3532\n",
            "Epoch 6/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 2.1979 - accuracy: 0.4147\n",
            "Epoch 7/100\n",
            "932/932 [==============================] - 126s 136ms/step - loss: 1.8938 - accuracy: 0.4812\n",
            "Epoch 8/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 1.6091 - accuracy: 0.5525\n",
            "Epoch 9/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 1.3446 - accuracy: 0.6254\n",
            "Epoch 10/100\n",
            "932/932 [==============================] - 127s 136ms/step - loss: 1.1106 - accuracy: 0.6866\n",
            "Epoch 11/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.9190 - accuracy: 0.7362\n",
            "Epoch 12/100\n",
            "932/932 [==============================] - 126s 136ms/step - loss: 0.7668 - accuracy: 0.7779\n",
            "Epoch 13/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.6491 - accuracy: 0.8119\n",
            "Epoch 14/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.5574 - accuracy: 0.8390\n",
            "Epoch 15/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.4864 - accuracy: 0.8608\n",
            "Epoch 16/100\n",
            "932/932 [==============================] - 126s 136ms/step - loss: 0.4312 - accuracy: 0.8749\n",
            "Epoch 17/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.3855 - accuracy: 0.8877\n",
            "Epoch 18/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.3482 - accuracy: 0.8984\n",
            "Epoch 19/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.3189 - accuracy: 0.9063\n",
            "Epoch 20/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.2918 - accuracy: 0.9139\n",
            "Epoch 21/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.2675 - accuracy: 0.9213\n",
            "Epoch 22/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.2470 - accuracy: 0.9265\n",
            "Epoch 23/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.2308 - accuracy: 0.9308\n",
            "Epoch 24/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.2167 - accuracy: 0.9358\n",
            "Epoch 25/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.2019 - accuracy: 0.9393\n",
            "Epoch 26/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.1881 - accuracy: 0.9448\n",
            "Epoch 27/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.1775 - accuracy: 0.9467\n",
            "Epoch 28/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.1664 - accuracy: 0.9496\n",
            "Epoch 29/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.1594 - accuracy: 0.9519\n",
            "Epoch 30/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.1498 - accuracy: 0.9537\n",
            "Epoch 31/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.1445 - accuracy: 0.9548\n",
            "Epoch 32/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.1361 - accuracy: 0.9577\n",
            "Epoch 33/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.1311 - accuracy: 0.9586\n",
            "Epoch 34/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.1246 - accuracy: 0.9605\n",
            "Epoch 35/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.1200 - accuracy: 0.9617\n",
            "Epoch 36/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.1172 - accuracy: 0.9624\n",
            "Epoch 37/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.1096 - accuracy: 0.9653\n",
            "Epoch 38/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.1062 - accuracy: 0.9660\n",
            "Epoch 39/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.1007 - accuracy: 0.9686\n",
            "Epoch 40/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.0979 - accuracy: 0.9699\n",
            "Epoch 41/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.0938 - accuracy: 0.9707\n",
            "Epoch 42/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.0871 - accuracy: 0.9733\n",
            "Epoch 43/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.0822 - accuracy: 0.9747\n",
            "Epoch 44/100\n",
            "932/932 [==============================] - 126s 136ms/step - loss: 0.0799 - accuracy: 0.9753\n",
            "Epoch 45/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.0785 - accuracy: 0.9761\n",
            "Epoch 46/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.0783 - accuracy: 0.9757\n",
            "Epoch 47/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.0706 - accuracy: 0.9781\n",
            "Epoch 48/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.0688 - accuracy: 0.9789\n",
            "Epoch 49/100\n",
            "932/932 [==============================] - 127s 136ms/step - loss: 0.0691 - accuracy: 0.9782\n",
            "Epoch 50/100\n",
            "932/932 [==============================] - 126s 136ms/step - loss: 0.0676 - accuracy: 0.9787\n",
            "Epoch 51/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.0601 - accuracy: 0.9814\n",
            "Epoch 52/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.0681 - accuracy: 0.9784\n",
            "Epoch 53/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.0582 - accuracy: 0.9815\n",
            "Epoch 54/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.0592 - accuracy: 0.9810\n",
            "Epoch 55/100\n",
            "932/932 [==============================] - 126s 136ms/step - loss: 0.0552 - accuracy: 0.9819\n",
            "Epoch 56/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.0534 - accuracy: 0.9820\n",
            "Epoch 57/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.0519 - accuracy: 0.9827\n",
            "Epoch 58/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.0506 - accuracy: 0.9833\n",
            "Epoch 59/100\n",
            "932/932 [==============================] - 126s 136ms/step - loss: 0.0542 - accuracy: 0.9821\n",
            "Epoch 60/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.0476 - accuracy: 0.9842\n",
            "Epoch 61/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.0461 - accuracy: 0.9852\n",
            "Epoch 62/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.0472 - accuracy: 0.9846\n",
            "Epoch 63/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.0525 - accuracy: 0.9833\n",
            "Epoch 64/100\n",
            "932/932 [==============================] - 127s 136ms/step - loss: 0.0510 - accuracy: 0.9833\n",
            "Epoch 65/100\n",
            "932/932 [==============================] - 127s 137ms/step - loss: 0.0464 - accuracy: 0.9848\n",
            "Epoch 66/100\n",
            "932/932 [==============================] - 127s 137ms/step - loss: 0.0439 - accuracy: 0.9861\n",
            "Epoch 67/100\n",
            "932/932 [==============================] - 128s 137ms/step - loss: 0.0438 - accuracy: 0.9864\n",
            "Epoch 68/100\n",
            "932/932 [==============================] - 128s 137ms/step - loss: 0.0454 - accuracy: 0.9846\n",
            "Epoch 69/100\n",
            "932/932 [==============================] - 128s 137ms/step - loss: 0.0428 - accuracy: 0.9854\n",
            "Epoch 70/100\n",
            "932/932 [==============================] - 128s 137ms/step - loss: 0.0460 - accuracy: 0.9835\n",
            "Epoch 71/100\n",
            "932/932 [==============================] - 127s 137ms/step - loss: 0.0442 - accuracy: 0.9853\n",
            "Epoch 72/100\n",
            "932/932 [==============================] - 128s 137ms/step - loss: 0.0410 - accuracy: 0.9851\n",
            "Epoch 73/100\n",
            "932/932 [==============================] - 127s 136ms/step - loss: 0.0407 - accuracy: 0.9858\n",
            "Epoch 74/100\n",
            "932/932 [==============================] - 127s 137ms/step - loss: 0.0404 - accuracy: 0.9863\n",
            "Epoch 75/100\n",
            "932/932 [==============================] - 127s 137ms/step - loss: 0.0368 - accuracy: 0.9878\n",
            "Epoch 76/100\n",
            "932/932 [==============================] - 127s 136ms/step - loss: 0.0367 - accuracy: 0.9879\n",
            "Epoch 77/100\n",
            "932/932 [==============================] - 127s 136ms/step - loss: 0.0348 - accuracy: 0.9885\n",
            "Epoch 78/100\n",
            "932/932 [==============================] - 128s 137ms/step - loss: 0.0339 - accuracy: 0.9886\n",
            "Epoch 79/100\n",
            "932/932 [==============================] - 127s 137ms/step - loss: 0.0333 - accuracy: 0.9891\n",
            "Epoch 80/100\n",
            "932/932 [==============================] - 128s 137ms/step - loss: 0.0364 - accuracy: 0.9874\n",
            "Epoch 81/100\n",
            "932/932 [==============================] - 127s 136ms/step - loss: 0.0327 - accuracy: 0.9890\n",
            "Epoch 82/100\n",
            "932/932 [==============================] - 127s 137ms/step - loss: 0.0325 - accuracy: 0.9893\n",
            "Epoch 83/100\n",
            "932/932 [==============================] - 128s 137ms/step - loss: 0.0318 - accuracy: 0.9897\n",
            "Epoch 84/100\n",
            "932/932 [==============================] - 127s 136ms/step - loss: 0.0301 - accuracy: 0.9903\n",
            "Epoch 85/100\n",
            "932/932 [==============================] - 127s 137ms/step - loss: 0.0294 - accuracy: 0.9904\n",
            "Epoch 86/100\n",
            "932/932 [==============================] - 127s 136ms/step - loss: 0.0298 - accuracy: 0.9904\n",
            "Epoch 87/100\n",
            "932/932 [==============================] - 128s 137ms/step - loss: 0.0314 - accuracy: 0.9900\n",
            "Epoch 88/100\n",
            "932/932 [==============================] - 127s 137ms/step - loss: 0.0291 - accuracy: 0.9906\n",
            "Epoch 89/100\n",
            "932/932 [==============================] - 127s 136ms/step - loss: 0.0418 - accuracy: 0.9866\n",
            "Epoch 90/100\n",
            "932/932 [==============================] - 127s 136ms/step - loss: 0.0301 - accuracy: 0.9901\n",
            "Epoch 91/100\n",
            "932/932 [==============================] - 127s 136ms/step - loss: 0.0266 - accuracy: 0.9914\n",
            "Epoch 92/100\n",
            "932/932 [==============================] - 127s 136ms/step - loss: 0.0265 - accuracy: 0.9918\n",
            "Epoch 93/100\n",
            "932/932 [==============================] - 127s 136ms/step - loss: 0.0262 - accuracy: 0.9920\n",
            "Epoch 94/100\n",
            "932/932 [==============================] - 127s 136ms/step - loss: 0.0262 - accuracy: 0.9917\n",
            "Epoch 95/100\n",
            "932/932 [==============================] - 128s 137ms/step - loss: 0.0258 - accuracy: 0.9917\n",
            "Epoch 96/100\n",
            "932/932 [==============================] - 127s 136ms/step - loss: 0.0256 - accuracy: 0.9916\n",
            "Epoch 97/100\n",
            "932/932 [==============================] - 127s 136ms/step - loss: 0.0292 - accuracy: 0.9907\n",
            "Epoch 98/100\n",
            "932/932 [==============================] - 127s 136ms/step - loss: 0.0275 - accuracy: 0.9914\n",
            "Epoch 99/100\n",
            "932/932 [==============================] - 127s 136ms/step - loss: 0.0256 - accuracy: 0.9918\n",
            "Epoch 100/100\n",
            "932/932 [==============================] - 128s 137ms/step - loss: 0.0231 - accuracy: 0.9927\n"
          ]
        }
      ],
      "source": [
        "model = Sequential()\n",
        "model.add(Bidirectional(LSTM(units=128,input_shape=(len(train_x[0]),1),return_sequences=True)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Bidirectional(LSTM(units=64,return_sequences=True)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Bidirectional(LSTM(units=32,return_sequences=True)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Bidirectional(LSTM(units=16,return_sequences=False)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(len(train_y[0]),activation=\"softmax\"))\n",
        "\n",
        "# Build the model\n",
        "model.build(input_shape=(None, len(train_x[0]), 1))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "sgd = SGD(learning_rate=0.001 ,momentum=0.9, nesterov=True)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "\n",
        "#fitting and saving the model\n",
        "hist = model.fit(np.array(train_x), np.array(train_y), epochs=100, batch_size=128, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Pn5pr4HjEmJ",
        "outputId": "38ff6c94-d6ef-4a1a-a890-3130ff589e47"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model created and saved\n"
          ]
        }
      ],
      "source": [
        "model.save( path_to_save_model+'chat_model_4', hist)\n",
        "print(\"model created and saved\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X71mU4ZljETL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "SeHAlQgIjDpx",
        "outputId": "de9b52ff-3d23-4d8d-d62c-1e55b5094734"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " bidirectional (Bidirection  (None, 391, 256)          133120    \n",
            " al)                                                             \n",
            "                                                                 \n",
            " batch_normalization (Batch  (None, 391, 256)          1024      \n",
            " Normalization)                                                  \n",
            "                                                                 \n",
            " bidirectional_1 (Bidirecti  (None, 391, 128)          164352    \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 391, 128)          0         \n",
            "                                                                 \n",
            " bidirectional_2 (Bidirecti  (None, 391, 64)           41216     \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 391, 64)           0         \n",
            "                                                                 \n",
            " batch_normalization_1 (Bat  (None, 391, 64)           256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " bidirectional_3 (Bidirecti  (None, 32)                10368     \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 32)                0         \n",
            "                                                                 \n",
            " batch_normalization_2 (Bat  (None, 32)                128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense (Dense)               (None, 113)               3729      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 354193 (1.35 MB)\n",
            "Trainable params: 353489 (1.35 MB)\n",
            "Non-trainable params: 704 (2.75 KB)\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "932/932 [==============================] - 145s 137ms/step - loss: 4.6558 - accuracy: 0.0336\n",
            "Epoch 2/100\n",
            "932/932 [==============================] - 129s 138ms/step - loss: 4.1301 - accuracy: 0.1049\n",
            "Epoch 3/100\n",
            "932/932 [==============================] - 128s 138ms/step - loss: 3.7473 - accuracy: 0.1478\n",
            "Epoch 4/100\n",
            "932/932 [==============================] - 128s 138ms/step - loss: 3.4378 - accuracy: 0.1868\n",
            "Epoch 5/100\n",
            "932/932 [==============================] - 129s 138ms/step - loss: 3.1494 - accuracy: 0.2339\n",
            "Epoch 6/100\n",
            "932/932 [==============================] - 129s 138ms/step - loss: 2.8716 - accuracy: 0.2799\n",
            "Epoch 7/100\n",
            "932/932 [==============================] - 129s 138ms/step - loss: 2.6211 - accuracy: 0.3216\n",
            "Epoch 8/100\n",
            "932/932 [==============================] - 129s 138ms/step - loss: 2.4007 - accuracy: 0.3596\n",
            "Epoch 9/100\n",
            "932/932 [==============================] - 130s 139ms/step - loss: 2.2027 - accuracy: 0.3998\n",
            "Epoch 10/100\n",
            "932/932 [==============================] - 130s 139ms/step - loss: 2.0192 - accuracy: 0.4404\n",
            "Epoch 11/100\n",
            "932/932 [==============================] - 129s 138ms/step - loss: 1.8442 - accuracy: 0.4803\n",
            "Epoch 12/100\n",
            "932/932 [==============================] - 129s 139ms/step - loss: 1.6993 - accuracy: 0.5156\n",
            "Epoch 13/100\n",
            "932/932 [==============================] - 129s 139ms/step - loss: 1.5580 - accuracy: 0.5511\n",
            "Epoch 14/100\n",
            "932/932 [==============================] - 129s 139ms/step - loss: 1.4138 - accuracy: 0.5903\n",
            "Epoch 15/100\n",
            "932/932 [==============================] - 129s 138ms/step - loss: 1.2874 - accuracy: 0.6236\n",
            "Epoch 16/100\n",
            "932/932 [==============================] - 129s 139ms/step - loss: 1.1761 - accuracy: 0.6543\n",
            "Epoch 17/100\n",
            "932/932 [==============================] - 129s 139ms/step - loss: 1.0703 - accuracy: 0.6818\n",
            "Epoch 18/100\n",
            "932/932 [==============================] - 129s 139ms/step - loss: 0.9511 - accuracy: 0.7191\n",
            "Epoch 19/100\n",
            "932/932 [==============================] - 129s 138ms/step - loss: 0.9754 - accuracy: 0.7076\n",
            "Epoch 20/100\n",
            "932/932 [==============================] - 129s 138ms/step - loss: 0.8029 - accuracy: 0.7618\n",
            "Epoch 21/100\n",
            "932/932 [==============================] - 129s 139ms/step - loss: 0.7363 - accuracy: 0.7807\n",
            "Epoch 22/100\n",
            "932/932 [==============================] - 129s 138ms/step - loss: 0.6862 - accuracy: 0.7938\n",
            "Epoch 23/100\n",
            "932/932 [==============================] - 130s 139ms/step - loss: 0.6902 - accuracy: 0.7925\n",
            "Epoch 24/100\n",
            "932/932 [==============================] - 129s 139ms/step - loss: 0.5948 - accuracy: 0.8207\n",
            "Epoch 25/100\n",
            "932/932 [==============================] - 129s 138ms/step - loss: 0.5565 - accuracy: 0.8309\n",
            "Epoch 26/100\n",
            "932/932 [==============================] - 129s 138ms/step - loss: 0.5487 - accuracy: 0.8341\n",
            "Epoch 27/100\n",
            "932/932 [==============================] - 129s 138ms/step - loss: 0.5008 - accuracy: 0.8481\n",
            "Epoch 28/100\n",
            "932/932 [==============================] - 129s 138ms/step - loss: 0.4716 - accuracy: 0.8583\n",
            "Epoch 29/100\n",
            "932/932 [==============================] - 129s 138ms/step - loss: 0.4422 - accuracy: 0.8665\n",
            "Epoch 30/100\n",
            "932/932 [==============================] - 129s 138ms/step - loss: 0.4298 - accuracy: 0.8706\n",
            "Epoch 31/100\n",
            "932/932 [==============================] - 129s 138ms/step - loss: 0.6385 - accuracy: 0.8112\n",
            "Epoch 32/100\n",
            "932/932 [==============================] - 129s 138ms/step - loss: 0.5284 - accuracy: 0.8354\n",
            "Epoch 33/100\n",
            "932/932 [==============================] - 129s 139ms/step - loss: 0.3811 - accuracy: 0.8849\n",
            "Epoch 34/100\n",
            "932/932 [==============================] - 129s 139ms/step - loss: 0.3357 - accuracy: 0.8987\n",
            "Epoch 35/100\n",
            "932/932 [==============================] - 129s 138ms/step - loss: 0.3031 - accuracy: 0.9090\n",
            "Epoch 36/100\n",
            "932/932 [==============================] - 129s 139ms/step - loss: 0.4038 - accuracy: 0.8822\n",
            "Epoch 37/100\n",
            "932/932 [==============================] - 129s 138ms/step - loss: 0.2943 - accuracy: 0.9132\n",
            "Epoch 38/100\n",
            "932/932 [==============================] - 129s 139ms/step - loss: 0.2569 - accuracy: 0.9228\n",
            "Epoch 39/100\n",
            "932/932 [==============================] - 129s 139ms/step - loss: 0.5749 - accuracy: 0.8398\n",
            "Epoch 40/100\n",
            "932/932 [==============================] - 129s 139ms/step - loss: 0.2453 - accuracy: 0.9276\n",
            "Epoch 41/100\n",
            "932/932 [==============================] - 129s 139ms/step - loss: 0.2448 - accuracy: 0.9261\n",
            "Epoch 42/100\n",
            "932/932 [==============================] - 129s 139ms/step - loss: 0.2230 - accuracy: 0.9331\n",
            "Epoch 43/100\n",
            "932/932 [==============================] - 129s 139ms/step - loss: 0.2210 - accuracy: 0.9344\n",
            "Epoch 44/100\n",
            "932/932 [==============================] - 129s 139ms/step - loss: 0.2366 - accuracy: 0.9296\n",
            "Epoch 45/100\n",
            "932/932 [==============================] - 129s 139ms/step - loss: 0.2673 - accuracy: 0.9200\n",
            "Epoch 46/100\n",
            "932/932 [==============================] - 130s 139ms/step - loss: 0.1893 - accuracy: 0.9443\n",
            "Epoch 47/100\n",
            "932/932 [==============================] - 129s 139ms/step - loss: 0.2203 - accuracy: 0.9347\n",
            "Epoch 48/100\n",
            "932/932 [==============================] - 130s 139ms/step - loss: 0.1895 - accuracy: 0.9435\n",
            "Epoch 49/100\n",
            "932/932 [==============================] - 130s 139ms/step - loss: 0.1884 - accuracy: 0.9439\n",
            "Epoch 50/100\n",
            "932/932 [==============================] - 130s 139ms/step - loss: 0.1903 - accuracy: 0.9426\n",
            "Epoch 51/100\n",
            "932/932 [==============================] - 129s 139ms/step - loss: 0.1809 - accuracy: 0.9463\n",
            "Epoch 52/100\n",
            "932/932 [==============================] - 129s 139ms/step - loss: 0.1735 - accuracy: 0.9479\n",
            "Epoch 53/100\n",
            "932/932 [==============================] - 130s 139ms/step - loss: 0.1785 - accuracy: 0.9456\n",
            "Epoch 54/100\n",
            "932/932 [==============================] - 129s 139ms/step - loss: 1.1548 - accuracy: 0.7643\n",
            "Epoch 55/100\n",
            "932/932 [==============================] - 129s 139ms/step - loss: 1.3068 - accuracy: 0.6001\n",
            "Epoch 56/100\n",
            "932/932 [==============================] - 129s 139ms/step - loss: 1.3468 - accuracy: 0.6587\n",
            "Epoch 57/100\n",
            "932/932 [==============================] - 129s 139ms/step - loss: 2.1607 - accuracy: 0.4120\n",
            "Epoch 58/100\n",
            "932/932 [==============================] - 130s 139ms/step - loss: 1.1217 - accuracy: 0.6412\n",
            "Epoch 59/100\n",
            "932/932 [==============================] - 130s 139ms/step - loss: 0.7630 - accuracy: 0.7456\n",
            "Epoch 60/100\n",
            "932/932 [==============================] - 129s 139ms/step - loss: 0.5916 - accuracy: 0.8025\n",
            "Epoch 61/100\n",
            "932/932 [==============================] - 129s 139ms/step - loss: 0.5010 - accuracy: 0.8335\n",
            "Epoch 62/100\n",
            "932/932 [==============================] - 129s 138ms/step - loss: 0.4271 - accuracy: 0.8602\n",
            "Epoch 63/100\n",
            "932/932 [==============================] - 130s 139ms/step - loss: 0.3834 - accuracy: 0.8765\n",
            "Epoch 64/100\n",
            "932/932 [==============================] - 129s 139ms/step - loss: 0.3504 - accuracy: 0.8858\n",
            "Epoch 65/100\n",
            "932/932 [==============================] - 129s 139ms/step - loss: 0.3145 - accuracy: 0.8993\n",
            "Epoch 66/100\n",
            "932/932 [==============================] - 129s 139ms/step - loss: 0.2871 - accuracy: 0.9081\n",
            "Epoch 67/100\n",
            "932/932 [==============================] - 130s 139ms/step - loss: 0.2809 - accuracy: 0.9097\n",
            "Epoch 68/100\n",
            "932/932 [==============================] - 129s 138ms/step - loss: 0.2499 - accuracy: 0.9205\n",
            "Epoch 69/100\n",
            "932/932 [==============================] - 129s 138ms/step - loss: 0.2296 - accuracy: 0.9271\n",
            "Epoch 70/100\n",
            "932/932 [==============================] - 130s 139ms/step - loss: 0.2267 - accuracy: 0.9280\n",
            "Epoch 71/100\n",
            "932/932 [==============================] - 129s 138ms/step - loss: 0.2176 - accuracy: 0.9305\n",
            "Epoch 72/100\n",
            "932/932 [==============================] - 130s 139ms/step - loss: 0.2333 - accuracy: 0.9261\n",
            "Epoch 73/100\n",
            "932/932 [==============================] - 129s 138ms/step - loss: 0.2141 - accuracy: 0.9325\n",
            "Epoch 74/100\n",
            "932/932 [==============================] - 129s 138ms/step - loss: 0.2009 - accuracy: 0.9362\n",
            "Epoch 75/100\n",
            "932/932 [==============================] - 129s 138ms/step - loss: 0.1968 - accuracy: 0.9375\n",
            "Epoch 76/100\n",
            "932/932 [==============================] - 129s 138ms/step - loss: 0.2325 - accuracy: 0.9285\n",
            "Epoch 77/100\n",
            "932/932 [==============================] - 129s 138ms/step - loss: 0.1756 - accuracy: 0.9442\n",
            "Epoch 78/100\n",
            "932/932 [==============================] - 129s 138ms/step - loss: 0.1708 - accuracy: 0.9459\n",
            "Epoch 79/100\n",
            "932/932 [==============================] - 129s 138ms/step - loss: 0.1660 - accuracy: 0.9473\n",
            "Epoch 80/100\n",
            "932/932 [==============================] - 129s 138ms/step - loss: 0.1923 - accuracy: 0.9391\n",
            "Epoch 81/100\n",
            "932/932 [==============================] - 129s 139ms/step - loss: 0.1514 - accuracy: 0.9520\n",
            "Epoch 82/100\n",
            "932/932 [==============================] - 129s 138ms/step - loss: 0.1617 - accuracy: 0.9489\n",
            "Epoch 83/100\n",
            "932/932 [==============================] - 130s 139ms/step - loss: 0.1406 - accuracy: 0.9558\n",
            "Epoch 84/100\n",
            "932/932 [==============================] - 130s 139ms/step - loss: 0.1413 - accuracy: 0.9550\n",
            "Epoch 85/100\n",
            "932/932 [==============================] - 130s 139ms/step - loss: 0.1374 - accuracy: 0.9566\n",
            "Epoch 86/100\n",
            "932/932 [==============================] - 129s 138ms/step - loss: 0.1682 - accuracy: 0.9468\n",
            "Epoch 87/100\n",
            "932/932 [==============================] - 128s 137ms/step - loss: 0.1444 - accuracy: 0.9540\n",
            "Epoch 88/100\n",
            "932/932 [==============================] - 128s 137ms/step - loss: 0.1271 - accuracy: 0.9593\n",
            "Epoch 89/100\n",
            "932/932 [==============================] - 128s 137ms/step - loss: 0.1262 - accuracy: 0.9602\n",
            "Epoch 90/100\n",
            "932/932 [==============================] - 128s 137ms/step - loss: 0.1205 - accuracy: 0.9622\n",
            "Epoch 91/100\n",
            "932/932 [==============================] - 128s 137ms/step - loss: 0.1179 - accuracy: 0.9625\n",
            "Epoch 92/100\n",
            "932/932 [==============================] - 128s 138ms/step - loss: 0.1276 - accuracy: 0.9594\n",
            "Epoch 93/100\n",
            "932/932 [==============================] - 128s 138ms/step - loss: 0.1176 - accuracy: 0.9630\n",
            "Epoch 94/100\n",
            "932/932 [==============================] - 128s 138ms/step - loss: 0.1193 - accuracy: 0.9627\n",
            "Epoch 95/100\n",
            "932/932 [==============================] - 128s 138ms/step - loss: 0.1077 - accuracy: 0.9659\n",
            "Epoch 96/100\n",
            "932/932 [==============================] - 129s 138ms/step - loss: 0.1090 - accuracy: 0.9656\n",
            "Epoch 97/100\n",
            "932/932 [==============================] - 129s 138ms/step - loss: 0.1111 - accuracy: 0.9654\n",
            "Epoch 98/100\n",
            "409/932 [============>.................] - ETA: 1:11 - loss: 0.1219 - accuracy: 0.9610"
          ]
        }
      ],
      "source": [
        "model = Sequential()\n",
        "model.add(Bidirectional(LSTM(units=128,input_shape=(len(train_x[0]),1),return_sequences=True)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Bidirectional(LSTM(units=64,return_sequences=True)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Bidirectional(LSTM(units=32,return_sequences=True)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Bidirectional(LSTM(units=16,return_sequences=False)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(len(train_y[0]),activation=\"softmax\"))\n",
        "\n",
        "# Build the model\n",
        "model.build(input_shape=(None, len(train_x[0]), 1))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "sgd = SGD(learning_rate=0.001 ,momentum=0.9, nesterov=True)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "\n",
        "#fitting and saving the model\n",
        "hist = model.fit(np.array(train_x), np.array(train_y), epochs=100, batch_size=128, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FVKnyKo1Ryg3"
      },
      "outputs": [],
      "source": [
        "model.save( path_to_save_model+'chat_model_5', hist)\n",
        "print(\"model created and saved\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gankztcp1D_a"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}